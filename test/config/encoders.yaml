
defaults: &defaults
  global:
    model_file: examples/output/<EXP>.mod
    out_file: examples/output/<EXP>.out
    err_file: examples/output/<EXP>.err
    default_layer_dim: 64
    dropout: 0.5
    weight_noise: 0.1
  train: !SimpleTrainingRegimen &defaults_train
    run_for_epochs: 2
    src_file: examples/data/head.ja
    trg_file: examples/data/head.en
    dev_tasks:
      - !LossEvalTask
        src_file: examples/data/head.ja
        ref_file: examples/data/head.en
  evaluate:
    - !AccuracyEvalTask
      eval_metrics: bleu,wer
      src_file: examples/data/head.ja
      ref_file: examples/data/head.en
      hyp_file: examples/output/<EXP>.test_hyp


exp1-lstm-encoder:
  << : *defaults
  model: !DefaultTranslator
    src_reader: !PlainTextReader {}
    trg_reader: !PlainTextReader {}
    src_embedder: !SimpleWordEmbedder
      emb_dim: 64
      word_dropout: 0.3
      vocab_size: 100 # TODO: Remove this line
    encoder: !BiLSTMSeqTransducer
      layers: 2
      input_dim: 64
    attender: !MlpAttender
      state_dim: 64
      hidden_dim: 64
      input_dim: 64
    trg_embedder: !SimpleWordEmbedder
      emb_dim: 64
      word_dropout: 0.3
      vocab_size: 100 # TODO: Remove this line
    decoder: !MlpSoftmaxDecoder
      layers: 1
      mlp_hidden_dim: 64
      input_feeding: True
      bridge: !CopyBridge {}
      vocab_size: 100 # TODO: Remove this line
    inference: !SimpleInference {}
exp2-residual-encoder:
  << : *defaults
  model: !DefaultTranslator
    src_reader: !PlainTextReader {}
    trg_reader: !PlainTextReader {}
    src_embedder: !SimpleWordEmbedder
      emb_dim: 64
      vocab_size: 100 # TODO: Remove this line
    encoder: !ResidualLSTMSeqTransducer
      layers: 2
      input_dim: 64
    attender: !MlpAttender
      state_dim: 64
      hidden_dim: 64
      input_dim: 64
    trg_embedder: !SimpleWordEmbedder
      emb_dim: 64
      vocab_size: 100 # TODO: Remove this line
    decoder: !MlpSoftmaxDecoder
      layers: 2
      mlp_hidden_dim: 64
      input_feeding: True
      bridge: !CopyBridge {}
      vocab_size: 100 # TODO: Remove this line
    inference: !SimpleInference {}
exp3-pyramidal-encoder:
  << : *defaults
  train: !SimpleTrainingRegimen
    << : *defaults_train
    batcher: !SrcBatcher
      batch_size: 5
      pad_src_to_multiple: 4
    dev_tasks: # TODO: Remove this segment by allowing batchers to default to train.batcher
      - !LossEvalTask
        batcher: !SrcBatcher
          batch_size: 5
          pad_src_to_multiple: 4
        src_file: examples/data/head.ja
        ref_file: examples/data/head.en
  model: !DefaultTranslator
    src_reader: !PlainTextReader {}
    trg_reader: !PlainTextReader {}
    src_embedder: !SimpleWordEmbedder
      emb_dim: 64
      vocab_size: 100 # TODO: Remove this line
    encoder: !PyramidalLSTMSeqTransducer
      layers: 3
      input_dim: 64
      hidden_dim: 64
    attender: !MlpAttender
      state_dim: 64
      hidden_dim: 64
      input_dim: 64
    trg_embedder: !SimpleWordEmbedder
      emb_dim: 64
      vocab_size: 100 # TODO: Remove this line
    decoder: !MlpSoftmaxDecoder
      layers: 2
      mlp_hidden_dim: 64
      input_feeding: True
      bridge: !CopyBridge {}
      vocab_size: 100 # TODO: Remove this line
    inference: !SimpleInference {}
exp4-modular-encoder:
  << : *defaults
  train: !SimpleTrainingRegimen
    << : *defaults_train
    batcher: !SrcBatcher
      batch_size: 5
      pad_src_to_multiple: 4
    dev_tasks: # TODO: Remove this segment by allowing batchers to default to train.batcher
      - !LossEvalTask
        batcher: !SrcBatcher
          batch_size: 5
          pad_src_to_multiple: 4
        src_file: examples/data/head.ja
        ref_file: examples/data/head.en
  model: !DefaultTranslator
    src_reader: !PlainTextReader {}
    trg_reader: !PlainTextReader {}
    src_embedder: !SimpleWordEmbedder
      emb_dim: 64
      vocab_size: 100 # TODO: Remove this line
    encoder: !ModularSeqTransducer
      modules:
      - !BiLSTMSeqTransducer
        input_dim: 64
        hidden_dim: 64
        layers: 1
        dropout: 0.1
      - !PyramidalLSTMSeqTransducer
        input_dim: 64
        hidden_dim: 64
        layers: 2
    attender: !MlpAttender
      state_dim: 64
      hidden_dim: 64
      input_dim: 64
    trg_embedder: !SimpleWordEmbedder
      emb_dim: 64
      vocab_size: 100 # TODO: Remove this line
    decoder: !MlpSoftmaxDecoder
      layers: 3
      mlp_hidden_dim: 64
      input_feeding: True
      bridge: !CopyBridge {}
      vocab_size: 100 # TODO: Remove this line
    inference: !SimpleInference {}

