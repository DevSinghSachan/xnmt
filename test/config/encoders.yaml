
defaults: &defaults
  global:
    model_file: examples/output/<EXP>.mod
    out_file: examples/output/<EXP>.out
    err_file: examples/output/<EXP>.err
    default_layer_dim: 64
    dropout: 0.5
    weight_noise: 0.1
  train: !SimpleTrainingRegimen &defaults_train
    run_for_epochs: 2
    src_file: examples/data/head.ja
    trg_file: examples/data/head.en
    dev_tasks:
      - !LossEvalTask
        src_file: examples/data/head.ja
        ref_file: examples/data/head.en
  evaluate:
    - !AccuracyEvalTask
      eval_metrics: bleu,wer
      src_file: examples/data/head.ja
      ref_file: examples/data/head.en
      hyp_file: examples/output/<EXP>.test_hyp


exp1-lstm-encoder:
  << : *defaults
  model: !DefaultTranslator
    src_reader: !PlainTextReader {}
    trg_reader: !PlainTextReader {}
    src_embedder: !SimpleWordEmbedder
      emb_dim: 64
      word_dropout: 0.3
      vocab_size: 10000
    encoder: !BiLSTMSeqTransducer
      layers: 2
      input_dim: 64
    attender: !MlpAttender
      state_dim: 64
      hidden_dim: 64
      input_dim: 64
    trg_embedder: !SimpleWordEmbedder
      emb_dim: 64
      word_dropout: 0.3
      vocab_size: 10000
    decoder: !MlpSoftmaxDecoder
      layers: 1
      mlp_hidden_dim: 64
      input_feeding: True
      bridge: !CopyBridge {}
      vocab_size: 10000
# exp2-residual-encoder:
#   << : *defaults
#   train: !SimpleTrainingRegimen
#     kwargs:
#       << : *defaults_train
#       model: !DefaultTranslator
#         src_embedder: !SimpleWordEmbedder
#           emb_dim: 64
#         encoder: !ResidualLSTMSeqTransducer
#           layers: 2
#           input_dim: 64
#         attender: !MlpAttender
#           state_dim: 64
#           hidden_dim: 64
#           input_dim: 64
#         trg_embedder: !SimpleWordEmbedder
#           emb_dim: 64
#         decoder: !MlpSoftmaxDecoder
#           layers: 2
#           mlp_hidden_dim: 64
#           input_feeding: True
#           bridge: !CopyBridge {}
# exp3-pyramidal-encoder:
#   << : *defaults
#   train: !SimpleTrainingRegimen
#     kwargs:
#       << : *defaults_train
#       batcher: !SrcBatcher
#         batch_size: 5
#         pad_src_to_multiple: 4
#       model: !DefaultTranslator
#         src_embedder: !SimpleWordEmbedder
#           emb_dim: 64
#         encoder: !PyramidalLSTMSeqTransducer
#           layers: 3
#           input_dim: 64
#           hidden_dim: 64
#         attender: !MlpAttender
#           state_dim: 64
#           hidden_dim: 64
#           input_dim: 64
#         trg_embedder: !SimpleWordEmbedder
#           emb_dim: 64
#         decoder: !MlpSoftmaxDecoder
#           layers: 2
#           mlp_hidden_dim: 64
#           input_feeding: True
#           bridge: !CopyBridge {}
# 
# 
# exp4-modular-encoder:
#   << : *defaults
#   train: !SimpleTrainingRegimen
#     kwargs:
#       << : *defaults_train
#       batcher: !SrcBatcher
#         batch_size: 5
#         pad_src_to_multiple: 2
#       model: !DefaultTranslator
#         src_embedder: !SimpleWordEmbedder
#           emb_dim: 64
#         encoder: !ModularSeqTransducer
#           modules:
#           - !BiLSTMSeqTransducer
#             input_dim: 64
#             hidden_dim: 64
#             layers: 1
#             dropout: 0.1
#           - !PyramidalLSTMSeqTransducer
#             input_dim: 64
#             hidden_dim: 64
#             layers: 2
#         attender: !MlpAttender
#           state_dim: 64
#           hidden_dim: 64
#           input_dim: 64
#         trg_embedder: !SimpleWordEmbedder
#           emb_dim: 64
#         decoder: !MlpSoftmaxDecoder
#           layers: 3
#           mlp_hidden_dim: 64
#           input_feeding: True
#           bridge: !CopyBridge {}

