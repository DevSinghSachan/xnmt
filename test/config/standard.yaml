# A standard training run, should almost never break
standard:
  experiment:
    model_file: examples/output/<EXP>.mod
    hyp_file: examples/output/<EXP>.hyp
    out_file: examples/output/<EXP>.out
    err_file: examples/output/<EXP>.err
    eval_metrics: bleu,wer
  model: !DefaultTranslator &main_model
    _xnmt_id: main_model
    glob:
      default_layer_dim: 32
    src_reader: !PlainTextReader {}
    trg_reader: !PlainTextReader {}
    src_embedder: !SimpleWordEmbedder
      vocab_size: 5000 # TODO: remove this line
      emb_dim: 64
    encoder: !BiLSTMSeqTransducer
      layers: 1
    attender: !MlpAttender
      state_dim: 64
      hidden_dim: 64
      input_dim: 64
    trg_embedder: !SimpleWordEmbedder
      vocab_size: 5000 # TODO: remove this line
      emb_dim: 64
    decoder: !MlpSoftmaxDecoder
      vocab_size: 5000 # TODO: remove this line
      layers: 1
      bridge: !CopyBridge {}
    inference: !SimpleInference
      len_norm_type: !PolynomialNormalization
        apply_during_search: true
        m: 1
  train: !SimpleTrainingRegimen
    run_for_epochs: 2
    model: *main_model # TODO: remove this line
    src_file: examples/data/head.ja
    trg_file: examples/data/head.en
    dev_tasks:
      - !LossEvalTask
        model: *main_model # TODO: remove this line
        src_file: examples/data/head.ja
        trg_file: examples/data/head.en
  evaluate:
    - !AccuracyEvalTask
      eval_metrics: bleu wer
      src_file: examples/data/head.ja
      trg_file: examples/data/head.en
