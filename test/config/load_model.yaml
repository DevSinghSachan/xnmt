exp1-pretrain-model: &exp1
  global:
    model_file: examples/output/<EXP>.mod
    out_file: examples/output/<EXP>.out
    err_file: examples/output/<EXP>.err
    default_layer_dim: 64
    dropout: 0.5
    weight_noise: 0.1
  model: !DefaultTranslator &main_model
    _xnmt_id: main_model
    src_embedder: !SimpleWordEmbedder
      emb_dim: 64
    encoder: !BiLSTMSeqTransducer
      layers: 2
      input_dim: 64
    attender: !MlpAttender
      state_dim: 64
      hidden_dim: 64
      input_dim: 64
    trg_embedder: !SimpleWordEmbedder
      emb_dim: 64
    decoder: !MlpSoftmaxDecoder
      layers: 1
      mlp_hidden_dim: 64
      input_feeding: True
      bridge: !CopyBridge {}
  train: !SimpleTrainingRegimen
    run_for_epochs: 2
    src_file: examples/data/head.ja
    trg_file: examples/data/head.en
    dev_task:
    - !AccuracyEvalTask
      model: *main_model # TODO: remove this line
      eval_metrics: bleu
      src_file: examples/data/head.ja
      ref_file: examples/data/head.en
      hyp_file: examples/output/<EXP>.dev_hyp
  evaluate:
    - !AccuracyEvalTask &acc_task
      model: *main_model # TODO: remove this line
      eval_metrics: bleu
      src_file: examples/data/head.ja
      ref_file: examples/data/head.en
      hyp_file: examples/output/<EXP>.test_hyp

# TODO: This is broken
exp2-finetune-model:
  << : *exp1
  train: !SimpleTrainingRegimen # load the pretrained model, but overwrite the optimizer to fine-tune with a different learning rate
    pretrained_model_file: examples/output/exp1-pretrain-model.mod
    corpus_parser: !BilingualCorpusParser
      src_reader: !PlainTextReader {}
      trg_reader: !PlainTextReader {}
      max_src_len: 15
      max_trg_len: 15
      training_corpus: !BilingualTrainingCorpus
        train_src: examples/data/head.ja
        train_trg: examples/data/head.en
        dev_src: examples/data/head.ja
        dev_trg: examples/data/head.en
